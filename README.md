# ğŸš€ Transformer Large Language Model

A simple yet powerful Transformer LLM implementation built with **PyTorch**, designed for clarity, modularity, and extensibility.

---

## âœ¨ Features

### **What This Code Does**

* ğŸ”¤ **Train BPE tokenizer from scratch** â€” Build your own vocabulary
* ğŸ”¥ **Load Mistral tokenizer** â€” Use a proven, production-ready BPE tokenizer
* ğŸ“Š **Dataset tokenization** â€” Efficient pre-processing for large-scale data
* ğŸ§  **Transformer training** â€” Train models from the ground up
* ğŸ’¬ **Text generation** â€” Generate text using trained checkpoints
* ğŸ“ **Supervised Fine-Tuning (SFT)** â€” Instruction / chat model fine-tuning
* ğŸ”€ **Mixture-of-Experts** â€” Efficient scaling via MoE routing
* ğŸ¯ **Multi-Head Latent Attention** â€” DeepSeek-inspired attention mechanism
* ğŸ–¥ï¸ **Multi-GPU training** â€” Distributed training using PyTorch DDP
* âš¡ **Mixed-precision training** â€” FP16/BF16 speed-ups with less memory

### **Current Limitations**

* âŒ No HuggingFace model loading
* âŒ No RLHF pipeline
* âŒ BPE-only tokenization
* âŒ No safetensors support
* âŒ Many advanced features still in progress

---

## ğŸ› ï¸ Quick Start

### **Prerequisites**

Install required dependencies:

```bash
pip install -r requirements.txt
```

> **Tip:** All scripts include sensible defaultsâ€”run them without arguments to get started fast.

---

## ğŸ”¤ Tokenizer Setup

Choose one of the two paths:

---

### **Option A: Use Mistral's Pre-trained Tokenizer**

Use the flag `--load_mistral_tokenizer` in training and generation steps.

**Setup:** Download only the `tokenizer.json` from **Mistral-Nemo-Base-2407** and place it in your project directory.

> ğŸ’¡ **Skip tokenizer training entirely.** Ideal for production or rapid prototyping.

---

### **Option B: Train Your Own Tokenizer**

```bash
python train_tokenizer.py
```

**Tokenizer Training Options:**

| Parameter                      |
| ------------------------------ |
| `--dataset_path_huggingface`   |
| `--dataset_sub_set`            |
| `--tokenizer_file_name`        |
| `--tokenizer_train_shard_size` |
| `--trust_remote_code`          |

**Pre-trained Resources (Custom Tokenizer Only):**

* ğŸ“¦ Custom BPE Tokenizer (Kaggle)
* ğŸ“Š Pre-tokenized FineWeb-Edu Dataset

> âš ï¸ These resources only work with the custom tokenizer, **not** Mistral.

---

## ğŸ“Š Training Pipeline

### **Pre-training Workflow**

---

#### **1ï¸âƒ£ Tokenize Your Dataset**

```bash
python tokenize_data.py
# or
python tokenize_data.py --load_mistral_tokenizer
```

**Data Tokenization Options:**

| Parameter                      |
| ------------------------------ |
| `--dataset_path_huggingface`   |
| `--dataset_sub_set`            |
| `--tokenizer_file_name`        |
| `--data_file_name`             |
| `--encoded_dataset_shard_size` |
| `--load_mistral_tokenizer`     |

**Pre-tokenized Dataset (Custom Only):** FineWeb-Edu 10B subset.

> âš ï¸ Only compatible with the custom tokenizer.

---

#### **2ï¸âƒ£ Train the Model**

```bash
python train.py --compile_model --use_autocast
# or
python train.py --compile_model --use_autocast --load_mistral_tokenizer
```

**Training Options:**

| Parameter                  |
| -------------------------- |
| `--steps`                  |
| `--eval_rate`              |
| `--eval_steps`             |
| `--save_rate`              |
| `--warm_up`                |
| `--total_batch_size`       |
| `--batch_size`             |
| `--seed`                   |
| `--promissed_flops`        |
| `--lr`                     |
| `--min_lr`                 |
| `--weight_decay`           |
| `--beta1`                  |
| `--beta2`                  |
| `--backend`                |
| `--save_file_name`         |
| `--data_file_name`         |
| `--tokenizer_file_name`    |
| `--dtype`                  |
| `--compile_model`          |
| `--use_autocast`           |
| `--load_mistral_tokenizer` |

---

#### **3ï¸âƒ£ Generate Text**

```bash
python generate.py --input_text "Hello" --num_tokens_to_generate 20 --compile_model
# or
python generate.py --input_text "Hello" --num_tokens_to_generate 20 --load_mistral_tokenizer --compile_model
```

**Generation Options:**

| Parameter                  |
| -------------------------- |
| `--input_text`             |
| `--num_tokens_to_generate` |
| `--temperature`            |
| `--top_p`                  |
| `--save_file_name`         |
| `--backend`                |
| `--tokenizer_file_name`    |
| `--compile_model`          |
| `--load_mistral_tokenizer` |

---

## ğŸ“ Supervised Fine-Tuning (SFT)

Fine-tune your model to follow instructions or engage in conversation.

### **When to Use SFT**

Perfect for:

* ğŸ’¬ Chatbots
* ğŸ“ Instruction models
* ğŸ¯ Domain-specific tuning
* ğŸ”„ Behavior alignment

---

### **SFT Workflow**

#### **1ï¸âƒ£ Prepare the SFT Dataset**

```bash
python tokenize_sft_data.py
# or
python tokenize_sft_data.py --load_mistral_tokenizer
```

**SFT Tokenization Options:**

| Parameter                        |
| -------------------------------- |
| `--sft_dataset_path_huggingface` |
| `--sft_dataset_sub_set`          |
| `--tokenizer_file_name`          |
| `--data_file_name`               |
| `--encoded_dataset_shard_size`   |
| `--load_mistral_tokenizer`       |

---

#### **2ï¸âƒ£ Fine-tune the Model**

```bash
python train_sft.py --compile_model --use_autocast
# or
python train_sft.py --load_mistral_tokenizer --compile_model --use_autocast
```

**SFT Training Options:**

| Parameter                  |
| -------------------------- |
| `--steps`                  |
| `--eval_rate`              |
| `--eval_steps`             |
| `--save_rate`              |
| `--warm_up`                |
| `--total_batch_size`       |
| `--batch_size`             |
| `--promissed_flops`        |
| `--lr`                     |
| `--min_lr`                 |
| `--weight_decay`           |
| `--beta1`                  |
| `--beta2`                  |
| `--backend`                |
| `--save_file_name`         |
| `--data_file_name`         |
| `--tokenizer_file_name`    |
| `--dtype`                  |
| `--compile_model`          |
| `--use_autocast`           |
| `--load_mistral_tokenizer` |

---

#### **3ï¸âƒ£ Test Your Instruction Model**

```bash
python generate.py --input_text "Python is" --num_tokens_to_generate 100 --save_file_name lilgpt_inst --compile_model
# or
python generate.py --input_text "Python is" --num_tokens_to_generate 100 --save_file_name lilgpt_inst --load_mistral_tokenizer --compile_model
```

---

## âš ï¸ Important Notes

### **Tokenizer Consistency**

You must use the **same tokenizer** for:

* Pre-training
* SFT
* Generation

Mixing tokenizers will break compatibility.

---

### **Which Tokenizer Should You Use?**

| Use Case                  | Recommended Option                        |
| ------------------------- | ----------------------------------------- |
| ğŸš€ Most users              | Custom tokenizer                          |
| âš¡ Quick testing           | Mistral OR custom                         |
| ğŸ­ Production              | Mistral tokenizer                         |
| ğŸ“ SFT / Chat models       | Mistral OR custom (better special tokens) |
| ğŸ”¬ Research / learning     | Custom tokenizer                          |
| ğŸŒ Non-English text        | Custom tokenizer                          |
| ğŸ“š Domain-specific content | Custom tokenizer                          |

---

## ğŸ—ºï¸ Architecture Highlights

* ğŸ”¥ DeepSeek Multi-Head Latent Attention
* âš–ï¸ Mixture-of-Experts
* âš¡ PyTorch Distributed Data Parallel
* ğŸ¯ Mixed Precision (FP16/BF16)

---

## ğŸ¤ Contributing

Contributions are welcome!

* ğŸ› Bug reports
* ğŸ’¡ Feature ideas
* ğŸ”§ Pull requests
* ğŸ“– Documentation improvements

---

## ğŸ“„ License

**GNU Affero General Public License (AGPL).**

---

## ğŸ™ Acknowledgments

* Mistral AI
* HuggingFace
* DeepSeek
* PyTorch Team
