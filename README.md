# üöÄ Transformer Large Language Model

A simple yet powerful Transformer LLM implementation built with PyTorch.

---

## ‚ú® Features

### What This Code Does
- üî§ **Train BPE tokenizer from scratch** - Build your own vocabulary
- üì• **Load Mistral tokenizer** - Use pre-trained Mistral BPE tokenizer
- üìä **Dataset tokenization** - Process data with pre-trained BPE tokenizer  
- üß† **Transformer training** - Train models from the ground up
- üí¨ **Text generation** - Generate text from pre-trained models
- üîÄ **Mixture-of-Experts** - Efficient scaling with MoE architecture
- üéØ **Multi-Head Latent Attention** - Advanced attention mechanism from DeepSeek
- üñ•Ô∏è **Multi-GPU training** - Distributed training with PyTorch DDP
- ‚ö° **Mixed-precision training** - Faster training with reduced memory usage

### Current Limitations
- ‚ùå No HuggingFace model loading support
- ‚ùå No RLHF fine-tuning capabilities  
- ‚ùå BPE tokenization only (no other algorithms)
- ‚ùå No safetensors support
- ‚ùå Many other features not yet implemented

---

## üõ†Ô∏è Quick Start

### Prerequisites
First, install the required dependencies:
```bash
pip install -r requirements.txt
```

> **Note:** All scripts have sensible default values set for their parameters. You can run them without any arguments to get started quickly, or customize the behavior using the options shown below.

---

## üî§ Tokenizer Setup

You have two options for tokenization:

### Option A: Use Mistral's Pre-trained Tokenizer (Recommended)
Skip the tokenizer training step and use the `--load_mistral_tokenizer=True` flag in subsequent steps. This leverages Mistral's proven vocabulary and is great for quick experimentation and production use.

**Setup:** Download only the `tokenizer.json` file (not the model weights) from [Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407/tree/main) and place it in your project directory.

> **üí° Tip:** Using Mistral's tokenizer means you can skip tokenizer training entirely and start directly with data tokenization!

### Option B: Train Your Own Tokenizer (For Custom Vocabularies)
```bash
python train_tokenizer.py
```

**Tokenizer Training Options:**

| Parameter                      |
| ------------------------------ |
| `--dataset_path_huggingface`   |
| `--dataset_sub_set`            |
| `--tokenizer_train_shard_size` |
| `--trust_remote_code`          |

**Pre-trained Resources (Custom Tokenizer Only):**
- üì¶ [Custom BPE Tokenizer on Kaggle](https://www.kaggle.com/models/rohankhanbd/bpetokenizer) - Download to skip training
- üìä [Pre-tokenized FineWeb-Edu Dataset](https://www.kaggle.com/datasets/rohankhanbd/half-tokenized-fineweb-edu-10b-subset) - Already tokenized with the custom tokenizer above

> **‚ö†Ô∏è Important:** These pre-trained resources only work with the custom tokenizer workflow (Option B). They are **not compatible** with Mistral's tokenizer.

---

## üìä Training Pipeline

### 1Ô∏è‚É£ Tokenize Your Dataset  
```bash
# With custom tokenizer
python tokenize_data.py

# OR with Mistral tokenizer
python tokenize_data.py --load_mistral_tokenizer=True
```

**Data Tokenization Options:**

| Parameter                      |
| ------------------------------ |
| `--dataset_path_huggingface`   |
| `--dataset_sub_set`            |
| `--data_file_name`             |
| `--encoded_dataset_shard_size` |
| `--load_mistral_tokenizer`     |

**Pre-tokenized Dataset (Custom Tokenizer Only):**
- üìä [FineWeb-Edu 10B Subset](https://www.kaggle.com/datasets/rohankhanbd/half-tokenized-fineweb-edu-10b-subset) - Skip tokenization if using the custom tokenizer

> **‚ö†Ô∏è Note:** The pre-tokenized dataset above only works with the custom tokenizer, not with `--load_mistral_tokenizer`.

### 2Ô∏è‚É£ Train the Model
```bash
# With custom tokenizer
python train.py

# OR with Mistral tokenizer
python train.py --load_mistral_tokenizer=True
```

**Training Options:**

| Parameter                  |
| -------------------------- |
| `--steps`                  |
| `--eval_rate`              |
| `--eval_steps`             |
| `--save_rate`              |
| `--warm_up`                |
| `--total_batch_size`       |
| `--batch_size`             |
| `--seed`                   |
| `--lr`                     |
| `--min_lr`                 |
| `--weight_decay`           |
| `--beta1`                  |
| `--beta2`                  |
| `--backend`                |
| `--save_file_name`         |
| `--data_file_name`         |
| `--compile_model`          |
| `--load_mistral_tokenizer` |

### 3Ô∏è‚É£ Generate Text
```bash
# With custom tokenizer
python generate.py --input_text "Hello" --num_tokens_to_generate 20

# OR with Mistral tokenizer
python generate.py --input_text "Hello" --num_tokens_to_generate 20 --load_mistral_tokenizer=True
```

**Generation Options:**

| Parameter                  |
| -------------------------- |
| `--input_text`             |
| `--num_tokens_to_generate` |
| `--temperature`            |
| `--top_p`                  |
| `--save_file_name`         |
| `--load_mistral_tokenizer` |

**Examples:**
```bash
# With custom tokenizer
python generate.py --input_text "Once upon a time" --num_tokens_to_generate 50 --temperature 0.7 --top_p 0.9

# With Mistral tokenizer
python generate.py --input_text "Once upon a time" --num_tokens_to_generate 50 --temperature 0.7 --top_p 0.9 --load_mistral_tokenizer=True
```

---

## ‚ö†Ô∏è Important Notes

### Tokenizer Consistency
**Critical:** You must use the same tokenizer for training and generation that was used for data tokenization. 

- ‚úÖ If you tokenized data with `--load_mistral_tokenizer`, use it for training and generation
- ‚úÖ If you tokenized data with your custom tokenizer, don't use `--load_mistral_tokenizer` flag
- ‚ùå Mixing tokenizers will cause errors or produce gibberish output

### Which Tokenizer Should I Use?

| Use Case                | Recommendation                          |
| ----------------------- | --------------------------------------- |
| üöÄ Most users            | **Mistral tokenizer** (recommended)     |
| ‚ö° Quick experimentation | **Mistral tokenizer**                   |
| üè≠ Production use        | **Mistral tokenizer**                   |
| üî¨ Research & learning   | Custom tokenizer                        |
| üåç Non-English languages | Custom tokenizer trained on your data   |
| üìö Domain-specific text  | Custom tokenizer trained on domain data |

---

## üèóÔ∏è Architecture Highlights

- **üî• DeepSeek Multi-Head Latent Attention** - Enhanced attention mechanism
- **‚öñÔ∏è Mixture-of-Experts** - Scalable expert routing
- **‚ö° PyTorch DDP** - Efficient multi-GPU orchestration
- **üéØ Mixed Precision** - FP16/BF16 training optimization

---

## üìù Example Workflows

### Workflow 1: Using Mistral Tokenizer (Fastest)
```bash
# Step 1: Tokenize data
python tokenize_data.py --load_mistral_tokenizer=True

# Step 2: Train model
python train.py --load_mistral_tokenizer=True --steps 10000

# Step 3: Generate text
python generate.py --input_text "Hello world" --load_mistral_tokenizer=True
```

### Workflow 2: Using Custom Tokenizer (Most Flexible)
```bash
# Step 1: Train tokenizer
python train_tokenizer.py

# Step 2: Tokenize data
python tokenize_data.py

# Step 3: Train model
python train.py --steps 10000

# Step 4: Generate text
python generate.py --input_text "Hello world"
```

---