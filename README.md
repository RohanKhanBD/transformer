# ğŸš€ Transformer Large Language Model

A simple yet powerful Transformer LLM implementation built with PyTorch.

---

## âœ¨ Features

### What This Code Does
- ğŸ”¤ **Train BPE tokenizer from scratch** - Build your own vocabulary
- ğŸ“Š **Dataset tokenization** - Process data with pre-trained BPE tokenizer  
- ğŸ§  **Transformer training** - Train models from the ground up
- ğŸ’¬ **Text generation** - Generate text from pre-trained models
- ğŸ”€ **Mixture-of-Experts** - Efficient scaling with MoE architecture
- ğŸ¯ **Multi-Head Latent Attention** - Advanced attention mechanism from DeepSeek
- ğŸ–¥ï¸ **Multi-GPU training** - Distributed training with PyTorch DDP
- âš¡ **Mixed-precision training** - Faster training with reduced memory usage

### Current Limitations
- âŒ No HuggingFace model loading support
- âŒ No RLHF fine-tuning capabilities  
- âŒ BPE tokenization only (no other algorithms)
- âŒ No safetensors support
- âŒ Many other features not yet implemented

---

## ğŸ› ï¸ Quick Start

### Prerequisites
First, install the required dependencies:
```bash
pip install -r requirements.txt
```

### Training Pipeline

#### 1ï¸âƒ£ Train the Tokenizer
```bash
python train_tokenizer.py
```
*Or use our pre-trained tokenizer: [BPE Tokenizer on Kaggle](https://www.kaggle.com/models/rohankhanbd/bpetokenizer)*

#### 2ï¸âƒ£ Tokenize Your Dataset  
```bash
python tokenize_data.py
```
*Or use our pre-tokenized dataset: [FineWeb-Edu 10B Subset](https://www.kaggle.com/datasets/rohankhanbd/half-tokenized-fineweb-edu-10b-subset)*

#### 3ï¸âƒ£ Train the Model
```bash
python train.py
```

#### 4ï¸âƒ£ Generate Text
```bash
python generate.py "<input_text>" <num_tokens> <temperature> <top_p>
```

**Example:**
```bash
python generate.py "Hello" 20 0.7 0.9
```

---

## ğŸ“ Generation Parameters

| Parameter     | Description                  | Example         |
| ------------- | ---------------------------- | --------------- |
| `input_text`  | Starting text for generation | `"Hello world"` |
| `num_tokens`  | Number of tokens to generate | `50`            |
| `temperature` | Randomness control (0.0-2.0) | `0.7`           |
| `top_p`       | Nucleus sampling threshold   | `0.9`           |

---

## ğŸ—ï¸ Architecture Highlights

- **ğŸ”¥ DeepSeek Multi-Head Latent Attention** - Enhanced attention mechanism
- **âš–ï¸ Mixture-of-Experts** - Scalable expert routing
- **âš¡ PyTorch DDP** - Efficient multi-GPU orchestration
- **ğŸ¯ Mixed Precision** - FP16/BF16 training optimization

---